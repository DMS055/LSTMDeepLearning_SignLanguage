{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8f323f",
   "metadata": {},
   "source": [
    "# Sign Language Recognition Using LSTM DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.5.0 tensorflow-gpu==2.5.0 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970be03",
   "metadata": {},
   "source": [
    "## Installing and importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72578fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28764d92",
   "metadata": {},
   "source": [
    "## Marking the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025241da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4cecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_det(image, model):\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    res = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71532c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def styled_landmarks(image, model):\n",
    "    # Face\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, res.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "        mp_drawing.DrawingSpec(color=(80, 110, 10),\n",
    "                               thickness=1,\n",
    "                               circle_radius=1),\n",
    "        mp_drawing.DrawingSpec(color=(80, 256, 121),\n",
    "                               thickness=1,\n",
    "                               circle_radius=1))\n",
    "    # Pose\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, res.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(80, 22, 10),\n",
    "                               thickness=2,\n",
    "                               circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(80, 44, 121),\n",
    "                               thickness=2,\n",
    "                               circle_radius=2))\n",
    "    # Left hand\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, res.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(121, 22, 76),\n",
    "                               thickness=2,\n",
    "                               circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(121, 44, 250),\n",
    "                               thickness=2,\n",
    "                               circle_radius=2))\n",
    "    # Right hand\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, res.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(245, 117, 66),\n",
    "                               thickness=2,\n",
    "                               circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(245, 66, 230),\n",
    "                               thickness=2,\n",
    "                               circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "# Setting the model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,\n",
    "                          min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read capture\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Detect\n",
    "        image, res = mediapipe_det(frame, holistic)\n",
    "\n",
    "        # Draw\n",
    "        styled_landmarks(image, res)\n",
    "\n",
    "        cv.imshow(\"Camera Feed\", image)\n",
    "\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76d545",
   "metadata": {},
   "source": [
    "## Extracting the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2470563",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for resV in res.pose_landmarks.landmark:\n",
    "    test = np.array([resV.x, resV.y, resV.z, resV.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed115736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(res):\n",
    "    # 33 * 4 = 33 landmarks, 4 values each (includes resV.visibility)\n",
    "    pose = np.array([[resV.x, resV.y, resV.z, resV.visibility]\n",
    "                     for resV in res.pose_landmarks.landmark\n",
    "                     ]).flatten() if res.pose_landmarks else np.zeros(132)\n",
    "    # 468 * 3 = 468 landmarks, 3 values each\n",
    "    face = np.array([[resV.x, resV.y, resV.z]\n",
    "                     for resV in res.face_landmarks.landmark\n",
    "                     ]).flatten() if res.face_landmarks else np.zeros(1404)\n",
    "    lHand = np.array([[resV.x, resV.y, resV.z]\n",
    "                      for resV in res.left_hand_landmarks.landmark\n",
    "                      ]).flatten() if res.left_hand_landmarks else np.zeros(63)\n",
    "    rHand = np.array([\n",
    "        [resV.x, resV.y, resV.z] for resV in res.right_hand_landmarks.landmark\n",
    "    ]).flatten() if res.right_hand_landmarks else np.zeros(63)\n",
    "    # 21 * 3 = 21 landmarks, 3 values each\n",
    "    return np.concatenate([pose, face, lHand, rHand])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c46bc",
   "metadata": {},
   "source": [
    "## Collection file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('MP_Data')\n",
    "actions = np.array(['hello', \"thanks\", \"i love you\"])\n",
    "\n",
    "# 30 seqences\n",
    "no_seq = 30\n",
    "\n",
    "# Every sequence 30 frames long\n",
    "seq_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for seq in range(no_seq):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(seq)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee1c89",
   "metadata": {},
   "source": [
    "## Collecting values from keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "# Setting the model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,\n",
    "                          min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        for seq in range(no_seq):\n",
    "            for frame_num in range(seq_length):\n",
    "\n",
    "                # Read capture\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Detect\n",
    "                image, res = mediapipe_det(frame, holistic)\n",
    "\n",
    "                # Draw\n",
    "                styled_landmarks(image, res)\n",
    "\n",
    "                # Wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv.putText(image, 'Starting collection', (120, 200),\n",
    "                               cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4,\n",
    "                               cv.LINE_AA)\n",
    "                    cv.putText(\n",
    "                        image,\n",
    "                        'Collecting frames for {} Video Number {}'.format(\n",
    "                            action, seq), (15, 12), cv.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (0, 0, 255), 1, cv.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv.imshow('OpenCV Feed', image)\n",
    "                    cv.waitKey(500)\n",
    "                else:\n",
    "                    cv.putText(\n",
    "                        image,\n",
    "                        'Collecting frames for: {}. Seq. Number: {}.'.format(\n",
    "                            action, seq), (15, 12), cv.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (0, 0, 255), 1, cv.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # Export\n",
    "                keypoints = extract(res)\n",
    "                np_path = os.path.join(DATA_PATH, action, str(seq),\n",
    "                                       str(frame_num))\n",
    "                np.save(np_path, keypoints)\n",
    "\n",
    "                if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c0e66",
   "metadata": {},
   "source": [
    "## Preprocess and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd494c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad98a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
